{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap YouTube Comments\n",
    "\n",
    "#### This notebook will demonstrate how to scrap YouTube comments.\n",
    "\n",
    "#### Author: Priyanka Dave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img.png\" title=\"Scrap YouTube Comments\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datakund\n",
      "  Using cached datakund-1.5.9.tar.gz (29.3 MB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "Building wheels for collected packages: datakund\n",
      "  Building wheel for datakund (setup.py): started\n",
      "  Building wheel for datakund (setup.py): finished with status 'done'\n",
      "  Created wheel for datakund: filename=datakund-1.5.9-py3-none-any.whl size=29350927 sha256=c65ad04ce74d48393d61e9e3d2e634c39bb95a68bbd019fc9360b8da9f3082cf\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\ba\\b7\\aa\\13fdc5358ac950a161a3f702940c29fe01d6eafb53ee2e9a4d\n",
      "Successfully built datakund\n",
      "Installing collected packages: urllib3, idna, chardet, certifi, tqdm, requests, datakund\n",
      "Successfully installed certifi-2020.12.5 chardet-4.0.0 datakund-1.5.9 idna-2.10 requests-2.25.1 tqdm-4.59.0 urllib3-1.26.4\n",
      "Collecting youtube_comment_scraper\n",
      "  Using cached youtube-comment-scraper-1.2.2.tar.gz (2.9 kB)\n",
      "Requirement already satisfied: requests in d:\\git\\web-scraping\\ws\\lib\\site-packages (from youtube_comment_scraper) (2.25.1)\n",
      "Requirement already satisfied: datakund in d:\\git\\web-scraping\\ws\\lib\\site-packages (from youtube_comment_scraper) (1.5.9)\n",
      "Requirement already satisfied: tqdm in d:\\git\\web-scraping\\ws\\lib\\site-packages (from datakund->youtube_comment_scraper) (4.59.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\git\\web-scraping\\ws\\lib\\site-packages (from requests->youtube_comment_scraper) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\git\\web-scraping\\ws\\lib\\site-packages (from requests->youtube_comment_scraper) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\git\\web-scraping\\ws\\lib\\site-packages (from requests->youtube_comment_scraper) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\git\\web-scraping\\ws\\lib\\site-packages (from requests->youtube_comment_scraper) (2.10)\n",
      "Building wheels for collected packages: youtube-comment-scraper\n",
      "  Building wheel for youtube-comment-scraper (setup.py): started\n",
      "  Building wheel for youtube-comment-scraper (setup.py): finished with status 'done'\n",
      "  Created wheel for youtube-comment-scraper: filename=youtube_comment_scraper-1.2.2-py3-none-any.whl size=2373 sha256=cad568a0122dba2304cf3b8c3a45764c0f439510909e1016032ec8700d566a01\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\be\\d3\\85\\9b64a756a8201d5bb8a590f825b65c9b267f25630050d5f3ad\n",
      "Successfully built youtube-comment-scraper\n",
      "Installing collected packages: youtube-comment-scraper\n",
      "Successfully installed youtube-comment-scraper-1.2.2\n"
     ]
    }
   ],
   "source": [
    "! pip install datakund\n",
    "! pip install youtube_comment_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import required packages:\n",
    "\n",
    "#### This will open your web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_comment_scraper import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Open YouTube video link and go to the comment section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube.open(\"https://www.youtube.com/watch?v=rSDy5AdfRDI\")\n",
    "youtube.keypress(\"pagedown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Start scraping :\n",
    "\n",
    "#### After executing this piece of code, you need to open web browser where your video is already opened.\n",
    "#### It will automatically perform \"Page Down\" and scrap new comments until all comments get scraped. You don't need to perform any actions. Just stay on that page and wait for process to complete.\n",
    "#### This step may take time as it depends on the number of comments available on video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████████| 200.0/200 [00:16<00:00, 12.06it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████████| 200.0/200 [00:27<00:00,  7.40it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████████| 200.0/200 [00:29<00:00,  6.84it/s]\n",
      "Progress:  71%|███████████████████████████████████████▊                | 142.1978021978022/200 [00:27<00:14,  4.12it/s]\n",
      "Progress: 203.9072039072039it [00:45,  3.48it/s]                                                                       \u001b[A\n",
      "Progress: 209.46275946275944it [00:47,  3.39it/s]                      | 72.82051282051283/200 [00:16<00:28,  4.41it/s]\u001b[A\n",
      "Progress: 216.12942612942612it [00:49,  3.35it/s]                      | 78.37606837606839/200 [00:18<00:29,  4.14it/s]\u001b[A\n",
      "Progress: 221.68498168498166it [00:52,  3.24it/s]                      | 83.93162393162395/200 [00:20<00:29,  3.87it/s]\u001b[A\n",
      "Progress: 227.2405372405372it [00:54,  3.13it/s]                        | 90.5982905982906/200 [00:22<00:29,  3.72it/s]\u001b[A\n",
      "Progress: 232.79609279609278it [00:56,  3.03it/s]                      | 96.15384615384616/200 [00:24<00:29,  3.47it/s]\u001b[A\n",
      "Progress: 238.35164835164832it [00:58,  2.94it/s]                     | 101.70940170940173/200 [00:27<00:30,  3.26it/s]\u001b[A\n",
      "Progress: 243.9072039072039it [01:00,  2.87it/s]                      | 107.26495726495729/200 [00:29<00:29,  3.10it/s]\u001b[A\n",
      "Progress: 249.46275946275944it [01:02,  2.81it/s]                     | 112.82051282051282/200 [00:31<00:29,  2.98it/s]\u001b[A\n",
      "Progress: 255.01831501831498it [01:04,  2.76it/s]                     | 118.37606837606837/200 [00:33<00:28,  2.88it/s]\u001b[A\n",
      "Progress: 260.5738705738705it [01:06,  2.73it/s]                      | 123.93162393162393/200 [00:35<00:27,  2.81it/s]\u001b[A\n",
      "Progress: 265.018315018315it [01:08,  2.55it/s] ███▎                   | 129.4871794871795/200 [00:37<00:25,  2.77it/s]\u001b[A\n",
      "Progress: 270.5738705738705it [01:10,  2.57it/s]███▊                  | 133.93162393162393/200 [00:39<00:25,  2.57it/s]\u001b[A\n",
      "Progress: 276.12942612942607it [01:13,  2.60it/s]█████                 | 139.4871794871795/200 [00:41<00:23,  2.60it/s]\u001b[A\n",
      "Progress: 280.5738705738705it [01:15,  2.45it/s] █████▌               | 143.93162393162393/200 [00:43<00:22,  2.45it/s]\u001b[A\n",
      "Progress: 285.018315018315it [01:17,  2.35it/s] ████████▊              | 149.4871794871795/200 [00:46<00:20,  2.50it/s]\u001b[A\n",
      "Progress: 332.19780219780216it [01:19,  4.19it/s]████████▎            | 153.93162393162393/200 [00:48<00:19,  2.38it/s]\u001b[A\n",
      "\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████████| 200.0/200 [00:50<00:00,  3.99it/s]\u001b[A\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████████| 200.0/200 [01:17<00:00,  2.58it/s]\n",
      "Progress:   0%|                                                                                | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "currentpagesource=youtube.get_page_source()\n",
    "lastpagesource=''\n",
    "\n",
    "while(True):\n",
    "    if(lastpagesource==currentpagesource):\n",
    "        break\n",
    "        \n",
    "    lastpagesource=currentpagesource\n",
    "    response=youtube.video_comments()\n",
    "\n",
    "    for c in response['body']:\n",
    "        data.append(c)\n",
    "        \n",
    "    youtube.scroll()\n",
    "    currentpagesource=youtube.get_page_source()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Remove duplicate data and convert list to dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data) # conver list to dataframe\n",
    "\n",
    "df = df.replace('\\n',' ', regex=True) # clean data\n",
    "\n",
    "df = df[['Comment', 'Likes', 'Time']].drop_duplicates(keep=\"first\") # drop duplicate rows\n",
    "\n",
    "df.to_csv('data.csv',index=False) # export data to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Check data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just lost my job because of the pandemic as ...</td>\n",
       "      <td>272</td>\n",
       "      <td>6 months ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is literally the most down to earth person ...</td>\n",
       "      <td>137</td>\n",
       "      <td>9 months ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jack Ma's Top 10 Rules For Success  1. Learn f...</td>\n",
       "      <td>124</td>\n",
       "      <td>7 months ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you want to be Successful, learn to make a ...</td>\n",
       "      <td>165</td>\n",
       "      <td>10 months ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"we should be used to failure and earn the rig...</td>\n",
       "      <td>34</td>\n",
       "      <td>9 months ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment        Likes  \\\n",
       "0  I just lost my job because of the pandemic as ...       272      \n",
       "1  He is literally the most down to earth person ...       137      \n",
       "2  Jack Ma's Top 10 Rules For Success  1. Learn f...       124      \n",
       "3  If you want to be Successful, learn to make a ...       165      \n",
       "4  \"we should be used to failure and earn the rig...        34      \n",
       "\n",
       "            Time  \n",
       "0   6 months ago  \n",
       "1   9 months ago  \n",
       "2   7 months ago  \n",
       "3  10 months ago  \n",
       "4   9 months ago  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
